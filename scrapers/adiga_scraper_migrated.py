#!/usr/bin/env python3
"""
Adiga scraper for adiga.kr - University Admission Monitor
Migrated to inherit from BaseScraper for robust architecture
"""
import re
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import os
import sys

# Add the parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.base_scraper import BaseScraper


class AdigaScraper(BaseScraper):
    """Adiga.kr scraper implementation for the robust architecture."""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the Adiga scraper with configuration.
        
        Args:
            config: Configuration dictionary from sources.yaml
        """
        # Ensure config has required fields for BaseScraper
        base_config = {
            'name': config.get('name', 'adiga'),
            'base_url': config.get('base_url', 'https://www.adiga.kr'),
            'timeout': config.get('timeout', 30),
            'retry_attempts': config.get('retry_attempts', 3)
        }
        
        super().__init__(base_config)
        
        # Store original config for scraper-specific settings
        self.full_config = config
        
        # Scraper-specific configuration
        self.html_file_path = config.get('html_file_path', 'adiga_structure.html')
        self.max_articles = config.get('max_articles', 10)
        self.display_name = config.get('display_name', 'Adiga (ì–´ë””ê°€)')
        
        self.logger.info(f"Initialized {self.display_name} scraper")
    
    def fetch_articles(self) -> List[Dict[str, Any]]:
        """
        Fetch raw articles from Adiga.kr HTML structure.
        
        Returns:
            List[Dict]: Raw article data dictionaries
        """
        self.logger.info(f"Fetching articles from {self.html_file_path}")
        
        raw_articles = []
        
        try:
            # Read the HTML file
            if not os.path.exists(self.html_file_path):
                self.logger.error(f"HTML file not found: {self.html_file_path}")
                return self._get_fallback_articles()
            
            with open(self.html_file_path, 'r', encoding='utf-8') as f:
                html = f.read()
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Find article items - using the correct selector
            article_items = soup.select('ul.uctList02 li')
            self.logger.info(f"Found {len(article_items)} article items in HTML")
            
            # Parse each article
            for item in article_items[:self.max_articles]:
                try:
                    raw_article = self._extract_raw_article(item)
                    if raw_article:
                        raw_articles.append(raw_article)
                        
                except Exception as e:
                    self.logger.warning(f"Failed to parse article item: {e}")
                    continue
            
            self.logger.info(f"Successfully extracted {len(raw_articles)} raw articles")
            
        except Exception as e:
            self.logger.error(f"Fetch error: {e}")
            raw_articles = self._get_fallback_articles()
        
        return raw_articles
    
    def _extract_raw_article(self, item) -> Optional[Dict[str, Any]]:
        """
        Extract raw article data from a BeautifulSoup item.
        
        Args:
            item: BeautifulSoup element
            
        Returns:
            Optional[Dict]: Raw article data or None
        """
        # Find the anchor tag with onclick
        anchor = item.find('a', onclick=True)
        if not anchor:
            return None
        
        # Extract article ID from onclick
        onclick = anchor.get('onclick', '')
        match = re.search(r'fnDetailPopup\(["\'](\d+)["\']\)', onclick)
        if not match:
            return None
        
        article_id = match.group(1)
        
        # Get title
        title_elem = anchor.select_one('.uctCastTitle')
        if not title_elem:
            return None
        
        title = title_elem.get_text(strip=True).replace('newIcon', '').strip()
        
        # Get content preview
        content_elem = anchor.select_one('.content')
        content = content_elem.get_text(strip=True) if content_elem else ""
        
        # Get metadata
        info_elem = anchor.select_one('.info')
        metadata = ""
        if info_elem:
            spans = info_elem.find_all('span')
            metadata = " | ".join([span.get_text(strip=True) for span in spans])
        
        # Construct URL
        article_url = f"{self.base_url}/ArticleDetail.do?articleID={article_id}"
        
        # Combine content with metadata
        full_content = content
        if metadata:
            full_content += f"\nğŸ“… {metadata}"
        
        # Create raw article data
        return {
            'title': title,
            'content': full_content[:350],  # Limit content length
            'url': article_url,
            'article_id': article_id,
            'onclick_handler': onclick,
            'has_content': bool(content_elem),
            'has_metadata': bool(info_elem),
            'metadata': {
                'source': self.display_name,
                'scraped_at': datetime.now().isoformat()
            }
        }
    
    def parse_article(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parse raw article data into standardized format.
        
        Args:
            raw_data: Raw article data from fetch_articles()
            
        Returns:
            Standardized article dictionary with required fields
        """
        try:
            # Extract data from raw article
            article_id = raw_data.get('article_id', '')
            title = raw_data.get('title', '')
            content = raw_data.get('content', '')
            url = raw_data.get('url', '')
            metadata = raw_data.get('metadata', {})
            
            # Extract additional metadata
            publish_date = self._extract_publish_date(content)
            university = self._extract_university(title)
            
            # Create standardized article matching BaseScraper format
            article = {
                'id': f"adiga_{article_id}",
                'title': title,
                'content': content,
                'url': url,
                'source': self.name,
                'scraped_at': datetime.now().isoformat(),
                'published_date': publish_date.isoformat() if publish_date else None,
                'university': university,
                'department': self._extract_department(title, content),
                'metadata': {
                    **metadata,
                    'article_id': article_id,
                    'has_content': raw_data.get('has_content', False),
                    'has_metadata': raw_data.get('has_metadata', False),
                    'onclick_handler': raw_data.get('onclick_handler', '')
                }
            }
            
            self.logger.debug(f"Parsed article: {article_id}")
            return article
            
        except Exception as e:
            self.logger.error(f"Failed to parse raw article: {e}")
            # Return minimal valid article
            return {
                'id': f"adiga_error_{datetime.now().timestamp()}",
                'title': "Parse Error",
                'content': str(e),
                'url': '',
                'source': self.name,
                'scraped_at': datetime.now().isoformat(),
                'metadata': {'error': True, 'error_message': str(e)}
            }
    
    def _extract_publish_date(self, content: str) -> Optional[datetime]:
        """
        Extract publish date from content metadata.
        
        Args:
            content: Article content
            
        Returns:
            datetime object or None
        """
        # Simple date extraction - can be enhanced
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'(\d{4}-\d{2}-\d{2})',
            r'(\d{4})\.(\d{2})\.(\d{2})'
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                try:
                    date_str = match.group(0)
                    # Try to parse Korean date format
                    if 'ë…„' in date_str:
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                        return datetime(year, month, day)
                    elif '-' in date_str:
                        return datetime.fromisoformat(date_str)
                    elif '.' in date_str:
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                        return datetime(year, month, day)
                except (ValueError, IndexError):
                    continue
        
        return None
    
    def _extract_university(self, title: str) -> Optional[str]:
        """Extract university name from title."""
        # Common Korean university patterns
        univ_patterns = [
            (r'ì„œìš¸ëŒ€', 'ì„œìš¸ëŒ€í•™êµ'),
            (r'ì—°ì„¸ëŒ€', 'ì—°ì„¸ëŒ€í•™êµ'),
            (r'ê³ ë ¤ëŒ€', 'ê³ ë ¤ëŒ€í•™êµ'),
            (r'ì„±ê· ê´€ëŒ€', 'ì„±ê· ê´€ëŒ€í•™êµ'),
            (r'í•œì–‘ëŒ€', 'í•œì–‘ëŒ€í•™êµ'),
            (r'ì„œê°•ëŒ€', 'ì„œê°•ëŒ€í•™êµ'),
            (r'ì´í™”ì—¬ëŒ€', 'ì´í™”ì—¬ìëŒ€í•™êµ'),
            (r'ì¤‘ì•™ëŒ€', 'ì¤‘ì•™ëŒ€í•™êµ'),
            (r'ê²½í¬ëŒ€', 'ê²½í¬ëŒ€í•™êµ'),
            (r'í•œêµ­ì™¸ëŒ€', 'í•œêµ­ì™¸êµ­ì–´ëŒ€í•™êµ')
        ]
        
        for pattern, full_name in univ_patterns:
            if re.search(pattern, title):
                return full_name
        
        return None
    
    def _extract_department(self, title: str, content: str) -> Optional[str]:
        """Extract department from title and content."""
        # Check for department keywords in title and content
        text_to_check = f"{title} {content}".lower()
        
        department_keywords = {
            'music': ['ìŒì•…', 'ì‹¤ìš©ìŒì•…', 'ì„±ì•…', 'ì‘ê³¡'],
            'korean': ['í•œêµ­ì–´', 'êµ­ì–´', 'êµ­ì–´êµ­ë¬¸', 'êµ­ë¬¸í•™'],
            'english': ['ì˜ì–´', 'ì˜ì–´ì˜ë¬¸', 'ì˜ë¬¸í•™'],
            'liberal': ['ì¸ë¬¸', 'ì¸ë¬¸í•™', 'êµì–‘', 'êµì–‘êµìœ¡']
        }
        
        for dept, keywords in department_keywords.items():
            for keyword in keywords:
                if keyword in text_to_check:
                    return dept
        
        return None
    
    def _get_fallback_articles(self) -> List[Dict[str, Any]]:
        """
        Provide fallback articles for testing.
        
        Returns:
            List[Dict]: Fallback raw articles
        """
        self.logger.warning("Using fallback articles")
        return [
            {
                'title': '[ì…ì‹œì˜ ì •ì„] ì •ì‹œ ë“±ë¡ ì˜¤ëŠ˜ë¶€í„°â€¦ì´ì¤‘ ë“±ë¡ ìœ ì˜í•´ì•¼',
                'content': 'ì •ì‹œ ë“±ë¡ì´ ì˜¤ëŠ˜ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤. ëŒ€í•™ë³„ ë“±ë¡ ë§ˆê°ì¼ í™•ì¸.',
                'url': 'https://www.adiga.kr/ArticleDetail.do?articleID=26546',
                'article_id': '26546',
                'has_content': True,
                'has_metadata': True,
                'metadata': {
                    'source': self.display_name,
                    'scraped_at': datetime.now().isoformat(),
                    'is_fallback': True
                }
            }
        ]


# Legacy compatibility wrapper
class LegacyAdigaScraper:
    """Wrapper for backward compatibility with existing code."""
    
    def __init__(self):
        """Initialize with default config for legacy compatibility."""
        config = {
            'name': 'adiga',
            'base_url': 'https://www.adiga.kr',
            'html_file_path': 'adiga_structure.html',
            'max_articles': 10,
            'display_name': 'Adiga (ì–´ë””ê°€)',
            'timeout': 30,
            'retry_attempts': 3
        }
        self._scraper = AdigaScraper(config)
        self.source_name = self._scraper.display_name
    
    def scrape(self):
        """Legacy method name support."""
        return self._scraper.scrape()
    
    def find_new_programs(self, current_programs):
        """Legacy method name support."""
        return self._scraper.find_new_programs(current_programs)


if __name__ == "__main__":
    # Test the migrated scraper
    print("Testing AdigaScraper Migration...")
    print("=" * 60)
    
    # Create config matching sources.yaml
    config = {
        'name': 'adiga',
        'enabled': True,
        'base_url': 'https://www.adiga.kr',
        'scraper_class': 'AdigaScraper',
        'schedule': '*/30 * * * *',
        'priority': 1,
        'timeout': 30,
        'retry_attempts': 3,
        'html_file_path': 'adiga_structure.html',
        'max_articles': 5,
        'display_name': 'Adiga (ì–´ë””ê°€)'
    }
    
    try:
        # Test 1: Create instance
        print("\n1. Creating AdigaScraper instance...")
        scraper = AdigaScraper(config)
        print(f"   âœ… Successfully created {scraper.display_name}")
        print(f"   Name: {scraper.name}, Base URL: {scraper.base_url}")
        
        # Test 2: Fetch articles
        print("\n2. Testing fetch_articles()...")
        raw_articles = scraper.fetch_articles()
        print(f"   âœ… Fetched {len(raw_articles)} raw articles")
        
        if raw_articles:
            # Test 3: Parse article
            print("\n3. Testing parse_article()...")
            parsed = scraper.parse_article(raw_articles[0])
            print(f"   âœ… Parsed article ID: {parsed.get('id')}")
            print(f"   Title: {parsed.get('title')[:50]}...")
            print(f"   URL: {parsed.get('url')}")
            print(f"   Source: {parsed.get('source')}")
            
            # Test 4: Full scrape
            print("\n4. Testing full scrape() method...")
            articles = scraper.scrape()
            print(f"   âœ… Processed {len(articles)} articles")
            
            # Test 5: State management
            print("\n5. Testing state management...")
            if articles:
                save_result = scraper.save_detected(articles)
                print(f"   Save detected: {'âœ… Success' if save_result else 'âŒ Failed'}")
                
                previous = scraper.load_previous()
                print(f"   Load previous: {len(previous)} articles")
                
                new_programs = scraper.find_new_programs(articles)
                print(f"   New programs: {len(new_programs)}")
        
        # Test 6: Legacy compatibility
        print("\n6. Testing legacy compatibility...")
        legacy = LegacyAdigaScraper()
        legacy_result = legacy.scrape()
        print(f"   Legacy scrape: {len(legacy_result)} articles")
        
        print("\n" + "=" * 60)
        print("âœ… All migration tests passed!")
        print(f"\nNext steps:")
        print(f"1. Replace current scrapers/adiga_scraper.py with this migrated version")
        print(f"2. Test with: python -m scrapers.adiga_scraper")
        print(f"3. Update multi_monitor.py to use new architecture")
        
    except Exception as e:
        print(f"\nâŒ Migration test failed: {e}")
        import traceback
        traceback.print_exc()
