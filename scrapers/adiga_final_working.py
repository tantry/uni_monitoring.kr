#!/usr/bin/env python3
"""
Adiga Scraper - FINAL WORKING VERSION
Correct URL and proper content handling
"""
import re
import requests
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from datetime import datetime
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.base_scraper import BaseScraper


class AdigaWorkingScraper(BaseScraper):
    """Final working Adiga scraper"""
    
    def __init__(self, config: Dict[str, Any] = None):
        config = config or {
            'name': 'adiga',
            'base_url': 'https://www.adiga.kr',
            'html_file_path': 'adiga_structure.html',
            'max_articles': 10,
            'display_name': 'Adiga (ì–´ë””ê°€)',
            'timeout': 30,
            'retry_attempts': 3
        }
        
        base_config = {
            'name': config.get('name', 'adiga'),
            'base_url': config.get('base_url', 'https://www.adiga.kr'),
            'timeout': config.get('timeout', 30),
            'retry_attempts': config.get('retry_attempts', 3)
        }
        
        super().__init__(base_config)
        
        self.html_file_path = config.get('html_file_path', 'adiga_structure.html')
        self.max_articles = config.get('max_articles', 10)
        self.display_name = config.get('display_name', 'Adiga (ì–´ë””ê°€)')
        self.detail_url_base = "https://www.adiga.kr/uct/nmg/enw/newsDetail.do"
        
        self.logger.info(f"Initialized WORKING {self.display_name} scraper")
    
    def fetch_articles(self) -> List[Dict[str, Any]]:
        """Fetch articles with correct URL format"""
        self.logger.info("Fetching articles")
        
        if not os.path.exists(self.html_file_path):
            self.logger.error(f"HTML file not found: {self.html_file_path}")
            return self._get_fallback_articles()
        
        try:
            with open(self.html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            articles = []
            
            article_list = soup.find('ul', class_='uctList02')
            if not article_list:
                self.logger.error("Could not find article list")
                return self._get_fallback_articles()
            
            list_items = article_list.find_all('li')
            self.logger.info(f"Found {len(list_items)} articles")
            
            for li in list_items[:self.max_articles]:
                article_data = self._extract_article(li)
                if article_data:
                    articles.append(article_data)
            
            self.logger.info(f"Extracted {len(articles)} articles")
            return articles
            
        except Exception as e:
            self.logger.error(f"Error: {e}")
            return self._get_fallback_articles()
    
    def _extract_article(self, li_element) -> Optional[Dict[str, Any]]:
        """Extract article with CORRECT URL (prtlBbsId not prtlBssId)"""
        try:
            anchor = li_element.find('a', onclick=True)
            if not anchor:
                return None
            
            onclick = anchor.get('onclick', '')
            match = re.search(r'fnDetailPopup\("(\d+)"\)', onclick)
            if not match:
                return None
            
            article_id = match.group(1)
            
            # Extract title
            title_elem = anchor.find('p', class_='uctCastTitle')
            title = title_elem.get_text(strip=True) if title_elem else "No Title"
            title = title.replace('newIcon', '').strip()
            
            # Extract and clean content
            content_elem = anchor.find('span', class_='content')
            raw_content = content_elem.get_text(strip=True) if content_elem else ""
            content = self._clean_content(raw_content)
            
            # Extract date
            info_list = anchor.find('ul', class_='info')
            date_str = datetime.now().strftime('%Y-%m-%d')
            
            if info_list:
                info_items = info_list.find_all('li')
                if len(info_items) >= 2:
                    dept_spans = info_items[1].find_all('span')
                    if len(dept_spans) >= 2:
                        date_str = dept_spans[1].get_text(strip=True)
            
            # CORRECT URL: prtlBbsId not prtlBssId
            url = f"{self.detail_url_base}?prtlBbsId={article_id}"
            
            # Prepare title for Telegram
            telegram_title = self._prepare_for_telegram(title)
            
            return {
                'article_id': article_id,
                'title': title,
                'telegram_title': telegram_title,
                'content': content,
                'date': date_str,
                'url': url,
                'metadata': {
                    'source': self.display_name,
                    'scraped_at': datetime.now().isoformat(),
                    'scrape_method': 'list_snippet',
                    'article_id': article_id,
                    'has_content': bool(content),
                    'content_length': len(content),
                    'url_correct': True  # We're using correct URL format
                }
            }
            
        except Exception as e:
            self.logger.debug(f"Extraction error: {e}")
            return None
    
    def _clean_content(self, raw_content: str) -> str:
        """Clean the content snippet"""
        if not raw_content:
            return ""
        
        # Replace HTML entities
        content = raw_content.replace('&lt;', '<').replace('&gt;', '>')
        
        # Remove patterns
        patterns = [
            r'^<[^>]+>\s*',  # Source attribution at start
            r'\s*ë‰´ìŠ¤\s*ë‚´ìš© ì „ì²´ ë³´ê¸°$',
            r'ê¸°ì‚¬ ì œëª©\s*:\s*'
        ]
        
        for pattern in patterns:
            content = re.sub(pattern, '', content)
        
        return content.strip()
    
    def _prepare_for_telegram(self, title: str) -> str:
        """Prepare title for Telegram HTML mode"""
        # Escape single quotes for Telegram
        return title.replace("'", "&#x27;")
    
    def parse_article(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Parse article"""
        article_id = raw_data.get('article_id', '')
        title = raw_data.get('title', 'No Title')
        telegram_title = raw_data.get('telegram_title', title)
        content = raw_data.get('content', '')
        url = raw_data.get('url', '')
        
        # Extract metadata
        university = self._extract_university(title)
        department = self._extract_department(title, content)
        
        # Generate ID
        article_hash = f"adiga_{article_id}"
        
        # Update metadata
        metadata = raw_data.get('metadata', {})
        metadata.update({
            'university': university,
            'department': department,
            'telegram_title': telegram_title
        })
        
        return {
            'id': article_hash,
            'title': title,
            'telegram_title': telegram_title,
            'content': content,
            'url': url,
            'published_date': raw_data.get('date'),
            'university': university,
            'department': department if department else {'name': 'general'},
            'metadata': metadata,
            'source': self.name
        }
    
    def _extract_university(self, title: str) -> Optional[str]:
        """Extract university"""
        patterns = [
            (r'ì„œìš¸ëŒ€', 'ì„œìš¸ëŒ€í•™êµ'),
            (r'ì—°ì„¸ëŒ€', 'ì—°ì„¸ëŒ€í•™êµ'),
            (r'ê³ ë ¤ëŒ€', 'ê³ ë ¤ëŒ€í•™êµ'),
        ]
        
        for pattern, name in patterns:
            if re.search(pattern, title):
                return name
        return None
    
    def _extract_department(self, title: str, content: str) -> Optional[str]:
        """Extract department"""
        text = f"{title} {content}".lower()
        
        departments = {
            'music': ['ìŒì•…', 'ì‹¤ìš©ìŒì•…'],
            'korean': ['í•œêµ­ì–´', 'êµ­ì–´'],
            'english': ['ì˜ì–´', 'ì˜ì–´ì˜ë¬¸'],
        }
        
        for dept, keywords in departments.items():
            for keyword in keywords:
                if keyword in text:
                    return dept
        return None
    
    def _get_fallback_articles(self) -> List[Dict[str, Any]]:
        """Fallback with correct URL"""
        return [
            {
                'article_id': '26546',
                'title': '[ì…ì‹œì˜ ì •ì„] ì •ì‹œ ë“±ë¡ ì˜¤ëŠ˜ë¶€í„°â€¦ì´ì¤‘ ë“±ë¡ ìœ ì˜í•´ì•¼',
                'telegram_title': '[ì…ì‹œì˜ ì •ì„] ì •ì‹œ ë“±ë¡ ì˜¤ëŠ˜ë¶€í„°â€¦ì´ì¤‘ ë“±ë¡ ìœ ì˜í•´ì•¼',
                'content': 'ì •ì‹œ ë“±ë¡ì´ ì˜¤ëŠ˜ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤. ëŒ€í•™ë³„ ë“±ë¡ ë§ˆê°ì¼ í™•ì¸.',
                'url': 'https://www.adiga.kr/uct/nmg/enw/newsDetail.do?prtlBbsId=26546',
                'date': '2026-02-04',
                'metadata': {
                    'source': self.display_name,
                    'scraped_at': datetime.now().isoformat(),
                    'is_fallback': True
                }
            }
        ]
    
    def cleanup(self):
        pass


# Legacy compatibility wrapper
class LegacyAdigaScraper:
    def __init__(self):
        config = {
            'name': 'adiga',
            'base_url': 'https://www.adiga.kr',
            'html_file_path': 'adiga_structure.html',
            'max_articles': 10,
            'display_name': 'Adiga (ì–´ë””ê°€)',
            'timeout': 30,
            'retry_attempts': 3
        }
        self._scraper = AdigaWorkingScraper(config)
        self.source_name = self._scraper.display_name
        self.source_config = self._scraper.source_config
    
    def scrape(self):
        return self._scraper.scrape()
    
    def find_new_programs(self, current_programs):
        return self._scraper.find_new_programs(current_programs)
    
    def save_detected(self, programs):
        return self._scraper.save_detected(programs)


# Test
if __name__ == "__main__":
    print("Testing FINAL WORKING Adiga Scraper")
    print("=" * 60)
    
    scraper = AdigaWorkingScraper()
    
    try:
        articles = scraper.scrape()
        print(f"Scraped {len(articles)} articles")
        
        if articles:
            article = articles[0]
            print(f"\nğŸ“° First article:")
            print(f"Title: {article.get('title')}")
            print(f"Telegram title: {article.get('telegram_title')}")
            print(f"URL: {article.get('url')}")
            print(f"Content: {article.get('content', '')}")
            
            # Test URL
            import requests
            url = article.get('url')
            if url:
                print(f"\nğŸ”— Testing URL...")
                try:
                    response = requests.get(url, timeout=5)
                    print(f"Status: {response.status_code}")
                    if response.status_code == 200:
                        print("âœ… URL works!")
                        # Check it's the right parameter
                        if 'prtlBbsId=26546' in url:
                            print("âœ… Correct URL parameter (prtlBbsId)")
                except Exception as e:
                    print(f"âŒ URL error: {e}")
        
        print("\n" + "=" * 60)
        
    finally:
        scraper.cleanup()
