# University Admission Monitor (í•œêµ­ì–´: ëŒ€í•™ ì…í•™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ)

A Python-based monitoring system that scrapes Korean university admission announcements from various sources and sends Telegram alerts for new programs across multiple departments.

## âœ¨ Features

* **Multi-Source Monitoring**: Scrapes admission announcements from Korean education portals
* **Multi-Department Tracking**: Monitors announcements for:
  * **Music Departments** (ìŒì•…, ì‹¤ìš©ìŒì•…, ì„±ì•…, ì‘ê³¡)
  * **Korean Departments** (í•œêµ­ì–´, êµ­ì–´êµ­ë¬¸, êµ­ë¬¸í•™)
  * **English Departments** (ì˜ì–´, ì˜ì–´ì˜ë¬¸, ì˜ë¬¸í•™)
  * **Liberal Arts** (ì¸ë¬¸, ì¸ë¬¸í•™, êµì–‘êµìœ¡)
* **Real-time Alerts**: Sends Telegram notifications for new admission announcements
* **Intelligent Filtering**: Filters out irrelevant content using keyword matching
* **Duplicate Detection**: Prevents duplicate alerts using content hashing
* **Modular Architecture**: Easily extensible with new data sources
* **Robust Foundation**: Enterprise-grade architecture with proper error handling, logging, and configuration management

## ğŸ“‹ Prerequisites

* Python 3.8+
* Telegram Bot Token (from @BotFather)
* Telegram Channel/Chat ID

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/tantry/uni_monitoring.kr.git
cd uni_monitoring.kr
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Telegram

1. Create a Telegram bot via @BotFather
2. Get your `BOT_TOKEN`
3. Create a channel/group and get its `CHAT_ID`
4. Add the bot as an admin to your channel

### 4. Set Up Configuration

Copy the example configuration files:

```bash
cp config/config.example.yaml config/config.yaml
cp config/sources.example.yaml config/sources.yaml
cp config/filters.example.yaml config/filters.yaml
```

Edit `config/config.yaml` with your credentials:

```yaml
telegram:
  bot_token: "YOUR_BOT_TOKEN_HERE"
  chat_id: "YOUR_CHANNEL_CHAT_ID_HERE"  # Format: -1001234567890 for channels

database:
  path: "data/state.db"  # SQLite database for state management

logging:
  level: "INFO"
  file: "logs/monitor.log"
```

### 5. Run the Monitor

**Legacy mode (for backward compatibility):**
```bash
python multi_monitor.py
```

**New architecture mode:**
```bash
python core/monitor_engine.py
```

For periodic monitoring, use the included script:
```bash
./check_now.sh
```

## ğŸ—ï¸ Project Structure (Enhanced)

```
uni_monitoring.kr/
â”œâ”€â”€ config/                     # Configuration management
â”‚   â”œâ”€â”€ config.yaml            # Main configuration
â”‚   â”œâ”€â”€ sources.yaml           # Scraper source definitions
â”‚   â””â”€â”€ filters.yaml           # Department filtering rules
â”œâ”€â”€ core/                      # Core business logic
â”‚   â”œâ”€â”€ base_scraper.py        # Abstract base class for all scrapers
â”‚   â”œâ”€â”€ monitor_engine.py      # Main monitoring orchestrator
â”‚   â”œâ”€â”€ scraper_factory.py     # Factory for creating scraper instances
â”‚   â”œâ”€â”€ filter_engine.py       # Advanced filtering engine
â”‚   â””â”€â”€ state_manager.py       # Database-backed state management
â”œâ”€â”€ models/                    # Data models
â”‚   â””â”€â”€ article.py             # Article data class
â”œâ”€â”€ scrapers/                  # Scraper implementations
â”‚   â”œâ”€â”€ adiga_scraper.py       # Adiga.kr scraper (migrating to new architecture)
â”‚   â”œâ”€â”€ scraper_base.py        # Legacy base scraper (deprecated)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ notifiers/                 # Notification systems
â”‚   â”œâ”€â”€ telegram_notifier.py   # Telegram notification handler
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ filters/                   # Filter implementations
â”‚   â”œâ”€â”€ department_filter.py   # Department-based filtering
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ data/                      # Data storage
â”‚   â””â”€â”€ state.db               # SQLite database (auto-generated)
â”œâ”€â”€ logs/                      # Log files
â”‚   â””â”€â”€ monitor.log            # Application logs (auto-generated)
â”œâ”€â”€ multi_monitor.py           # Legacy monitoring orchestrator
â”œâ”€â”€ filters.py                 # Legacy filtering logic
â”œâ”€â”€ telegram_formatter.py      # Legacy Telegram formatter
â”œâ”€â”€ check_now.sh               # Monitoring script
â”œâ”€â”€ state.json                 # Legacy state tracking (auto-generated)
â””â”€â”€ README.md                  # This file
```

## ğŸ¯ Architecture Migration Status

### âœ… Completed Foundation
- **New Core Architecture**: `core/base_scraper.py`, `core/filter_engine.py`, `core/scraper_factory.py`
- **Enhanced Configuration**: YAML-based configs in `config/` directory
- **Data Models**: `models/article.py` for standardized article representation
- **State Management**: SQLite database for reliable state tracking

### ğŸ”„ In Progress
- **Scraper Migration**: Migrating `scrapers/adiga_scraper.py` to inherit from new `BaseScraper`
- **Legacy Integration**: Maintaining backward compatibility during transition

### ğŸ“‹ Pending
- **Notification System**: Migrating to new `notifiers/` architecture
- **Testing Framework**: Comprehensive test suite for new architecture
- **Documentation**: API documentation and contributor guidelines

## ğŸ”§ Configuration

### Adding New Departments

Edit `config/filters.yaml` to add new department keywords:

```yaml
departments:
  music:
    keywords: ['ìŒì•…', 'music', 'ì‹¤ìš©ìŒì•…', 'ì„±ì•…', 'ì‘ê³¡']
    description: "Music related departments"
  korean:
    keywords: ['í•œêµ­ì–´', 'êµ­ì–´', 'êµ­ì–´êµ­ë¬¸', 'êµ­ë¬¸í•™']
    description: "Korean language departments"
  english:
    keywords: ['ì˜ì–´', 'ì˜ì–´ì˜ë¬¸', 'ì˜ë¬¸í•™']
    description: "English language departments"
  liberal:
    keywords: ['ì¸ë¬¸', 'ì¸ë¬¸í•™', 'êµì–‘', 'êµì–‘êµìœ¡']
    description: "Liberal arts departments"
  
  # Add new departments here:
  # new_dept:
  #   keywords: ['keyword1', 'keyword2', 'keyword3']
  #   description: "Description of new department"
```

### Adding New Scrapers (New Architecture)

1. Create a new scraper in `scrapers/` inheriting from `BaseScraper`
2. Add source configuration in `config/sources.yaml`
3. Register the scraper in `core/scraper_factory.py`
4. Test with the new monitoring engine

### Adding New Scrapers (Legacy Architecture)

1. Create a new scraper in `scrapers/` following `scraper_base.py`
2. Add source configuration in `sources.py`
3. Import and initialize in `multi_monitor.py`

## ğŸ“Š Current Data Sources

* **Adiga (ì–´ë””ê°€)**: Korean university admission news portal
  * URL: https://adiga.kr
  * Status: âœ… Active (Legacy), ğŸ”„ Migrating to New Architecture
  * Coverage: General admission news and announcements

*More sources can be added easily through the modular scraper system*

## ğŸ¤– Telegram Integration

The system sends formatted Telegram messages with HTML formatting:

```
ğŸµ [ìƒˆ ì…í•™ ê³µê³ ] ì„œìš¸ëŒ€í•™êµ ìŒì•…í•™ê³¼ ì¶”ê°€ëª¨ì§‘

ğŸ“Œ ë¶€ì„œ/í•™ê³¼: music
ğŸ“ ë‚´ìš©: ì„œìš¸ëŒ€í•™êµ ìŒì•…í•™ê³¼ì—ì„œ 2026í•™ë…„ë„ ì¶”ê°€ëª¨ì§‘ì„ ì‹¤ì‹œí•©ë‹ˆë‹¤...
ğŸ”— ë§í¬: https://adiga.kr/ArticleDetail.do?articleID=26546

#ëŒ€í•™ì…ì‹œ #music
```

## ğŸ”„ GitHub Integration

### Secure Push Scripts

The repository includes secure scripts for automated GitHub pushes:

```bash
# Setup (one-time)
./setup_github.sh

# Manual push
./push_to_github.sh

# Automated daily push
./daily_push.sh
```

**Security Note**: All tokens and credentials are automatically excluded via `.gitignore`.

## ğŸ› Troubleshooting

### Common Issues

1. **No Telegram alerts**
   * Check `BOT_TOKEN` and `CHAT_ID` in `config/config.yaml`
   * Verify bot has admin permissions in channel
   * Check if announcements match department filters
   * Check logs in `logs/monitor.log` for errors

2. **No articles found**
   * Check scraper connectivity
   * Verify department keywords match actual announcements
   * Adjust filtering strictness in `config/filters.yaml`

3. **Duplicate alerts**
   * System uses database-backed duplicate detection
   * Check `data/state.db` for tracking history

4. **URL gives 404**
   * Adiga.kr may require session cookies
   * Articles use JavaScript navigation (`fnDetailPopup()`)
   * Consider using main site URL as fallback

### Debug Mode

Run with verbose output:

```bash
python core/monitor_engine.py --verbose
```

Or check the logs:

```bash
tail -f logs/monitor.log
```

## ğŸš§ Migration Guide

### For Developers: Migrating Legacy Scrapers

To migrate an existing scraper to the new architecture:

1. **Update scraper class** to inherit from `BaseScraper` instead of `ScraperBase`
2. **Implement required methods**:
   - `fetch_articles()`: Fetch raw articles from source
   - `parse_article()`: Parse raw data into `Article` model
   - `get_source_name()`: Return unique source identifier
3. **Update configuration** in `config/sources.yaml`
4. **Test** with the new monitoring engine

Example migration template:

```python
from core.base_scraper import BaseScraper
from models.article import Article

class NewAdigaScraper(BaseScraper):
    def __init__(self, config):
        super().__init__(config)
        self.base_url = "https://adiga.kr"
    
    def fetch_articles(self):
        # Implementation here
        pass
    
    def parse_article(self, raw_data):
        # Implementation here
        pass
    
    def get_source_name(self):
        return "adiga"
```

### For Users: Transition Period

During the migration period, both systems will work in parallel:
- **Legacy system**: `multi_monitor.py` â†’ Uses `state.json`
- **New system**: `core/monitor_engine.py` â†’ Uses `data/state.db`

## ğŸ“ˆ Future Enhancements

* Web dashboard for monitoring status
* Email notifications as alternative to Telegram
* More data sources (ê° ëŒ€í•™êµ ì…í•™ì²˜ ì§ì ‘ ìŠ¤í¬ë˜í•‘)
* Advanced filtering (ì§€ì—­, ì „í˜•ë³„, ëª¨ì§‘ì¸ì›)
* REST API for external integrations
* Containerization with Docker

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your scraper or improvements
4. Submit a pull request

### Development Notes

* **New contributions**: Follow the `core/base_scraper.py` interface
* **Maintenance contributions**: Follow existing patterns during transition
* **Department keywords**: Add to `config/filters.yaml`
* **Testing**: Test with `python test_integration.py` before submitting

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgements

* Built for Korean university admission monitoring community
* Uses BeautifulSoup for web scraping
* Telegram Bot API for notifications
* Community contributors for scraper implementations

---

**Last Updated**: 06 February 2026  
**Active Development**: Yes (Architecture Migration in Progress)  
**Primary Maintainer**: tantry  
**Telegram Support**: @ReiUniMonitor_bot (KR Uni Monitor)  
**Architecture Status**: âœ… Foundation Built, ğŸ”„ Scraper Migration in Progress
```

## Key Changes Made to README:

1. **Architecture Status Section**: Clearly shows what's completed (âœ…), in progress (ğŸ”„), and pending (ğŸ“‹)
2. **Updated Project Structure**: Reflects the new robust architecture you've built
3. **Migration Guide**: Separate section for developers migrating scrapers
4. **Transition Period**: Explains how both legacy and new systems work during migration
5. **Configuration Updates**: Shows YAML-based config instead of Python files
6. **Clear Path Forward**: Guides users on next steps for the migration
