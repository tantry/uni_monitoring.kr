# ğŸ“ University Admission Monitor

Automated Korean university admission announcement monitoring system with real-time Telegram notifications.

## âœ¨ Features

* **Multi-Source Monitoring**: Scrapes admission announcements from Korean education portals
* **Multi-Department Tracking**: Music, Korean, English, Liberal Arts departments
* **Real-time Alerts**: Telegram notifications for new announcements
* **Intelligent Filtering**: Keyword-based content filtering
* **Duplicate Detection**: SQLite-based deduplication
* **Selenium Support**: Handles JavaScript-rendered content and popups
* **Safe GitHub Integration**: Interactive review before pushing

## ğŸš€ Quick Start

### Prerequisites
```bash
# System packages (Manjaro/Arch)
sudo pacman -S python python-pip google-chrome-beta chromedriver

# Python packages
pip install requests beautifulsoup4 pyyaml selenium
```

### Installation
```bash
git clone https://github.com/tantry/uni_monitoring.kr.git
cd uni_monitoring.kr
pip install -r requirements.txt
```

### Configuration

1. **Telegram Setup**:
   - Create bot via @BotFather â†’ get `BOT_TOKEN`
   - Create channel â†’ get `CHAT_ID`
   - Add bot as admin to channel

2. **Edit config/config.yaml**:
```yaml
telegram:
  bot_token: "YOUR_BOT_TOKEN"
  chat_id: "YOUR_CHAT_ID"
database:
  path: "data/state.db"
logging:
  level: "INFO"
  file: "logs/monitor.log"
```

3. **Copy example configs**:
```bash
cp config/config.example.yaml config/config.yaml
cp config/sources.example.yaml config/sources.yaml
cp config/filters.example.yaml config/filters.yaml
```

### Run Monitor
```bash
# Test mode (no Telegram)
python3 core/monitor_engine.py --test

# Production mode
./check_now.sh

# Legacy mode
python3 multi_monitor.py
```

## ğŸ“‹ Project Structure
```
uni_monitoring.kr/
â”œâ”€â”€ config/                     # Configuration management
â”‚   â”œâ”€â”€ config.yaml            # Main configuration
â”‚   â”œâ”€â”€ sources.yaml           # Scraper source definitions
â”‚   â””â”€â”€ filters.yaml           # Department filtering rules
â”œâ”€â”€ core/                      # Core business logic
â”‚   â”œâ”€â”€ base_scraper.py        # Abstract base class for all scrapers
â”‚   â”œâ”€â”€ monitor_engine.py      # Main monitoring orchestrator
â”‚   â”œâ”€â”€ scraper_factory.py     # Factory for creating scraper instances
â”‚   â”œâ”€â”€ filter_engine.py       # Advanced filtering engine
â”‚   â””â”€â”€ state_manager.py       # Database-backed state management
â”œâ”€â”€ models/                    # Data models
â”‚   â””â”€â”€ article.py             # Article data class (Pydantic model)
â”œâ”€â”€ scrapers/                  # Site-specific scrapers
â”‚   â”œâ”€â”€ adiga_scraper.py       # âœ… Working (Selenium + popups)
â”‚   â”œâ”€â”€ scraper_template.py    # Template for new scrapers (â­ Use this!)
â”‚   â””â”€â”€ scraper_base.py        # Legacy base scraper (deprecated)
â”œâ”€â”€ notifiers/                 # Telegram notifications
â”‚   â””â”€â”€ telegram_notifier.py   # Telegram notification handler
â”œâ”€â”€ filters/                   # Filter implementations
â”‚   â””â”€â”€ department_filter.py   # Department-based filtering
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ utils/                     # Utility functions
â”œâ”€â”€ scripts/                   # Utility scripts
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ state.db               # SQLite database (auto-generated)
â”‚   â””â”€â”€ adiga/                 # Adiga-specific data
â”œâ”€â”€ logs/                      # Log files
â”‚   â””â”€â”€ monitor.log            # Application logs (auto-generated)
â”œâ”€â”€ multi_monitor.py           # Legacy monitoring orchestrator
â”œâ”€â”€ push_to_github_safe.sh     # Safe GitHub push with review steps
â”œâ”€â”€ setup_github_safe.sh       # Complete GitHub setup script
â”œâ”€â”€ push_to_github.sh          # Original GitHub push script
â”œâ”€â”€ check_now.sh               # Main monitoring script
â”œâ”€â”€ SCRAPER_DEVELOPMENT_GUIDE.md  # â­ Critical reference for scraper development
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Adiga Scraper - Technical Details

### The Challenge
Adiga.kr uses JavaScript popups (`fnDetailPopup()`) instead of direct article URLs. Articles cannot be accessed via GET requests.

### The Solution
**Selenium-based popup clicking**:
1. Initialize headless Chrome with proper ChromeDriver
2. Navigate to news page
3. Find links with `onclick="fnDetailPopup('ID')"`
4. Click each link to open popup
5. Extract content from popup
6. Close popup, repeat

### Key Code Pattern
```python
# Find popup links
popup_links = driver.find_elements(By.XPATH, "//a[contains(@onclick, 'fnDetailPopup')]")

for link in popup_links:
    # Click to open popup
    driver.execute_script("arguments[0].click();", link)
    time.sleep(2)
    
    # Extract popup content
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')
    popup = soup.find('div', class_='popCont')
    content = popup.get_text(strip=True) if popup else ""
```

## ğŸ“š Adding New Scrapers

**IMPORTANT**: Read `SCRAPER_DEVELOPMENT_GUIDE.md` first!

### Pre-Development Checklist
- [ ] Check if site requires cookies (Korean sites usually do)
- [ ] Check if content is JavaScript-rendered (view page source)
- [ ] Test if article links work directly or open popups
- [ ] Identify the pattern: Simple HTTP / Selenium / Popup clicking

### Steps
1. Copy `scrapers/scraper_template.py`
2. Follow the template's pattern selection
3. Test standalone: `python3 scrapers/your_scraper.py`
4. Add to `config/sources.yaml`
5. Test with monitor: `python3 core/monitor_engine.py --test`

### Scraper Template Structure
```python
"""
Template for new scraper implementations
"""
import logging
from typing import List, Dict, Optional
from core.base_scraper import BaseScraper
from models.article import Article

logger = logging.getLogger(__name__)

class NewSourceScraper(BaseScraper):
    """Scraper for [Source Name]"""
    
    def __init__(self, config: dict):
        super().__init__(config)
        self.base_url = "https://example.com"
        self.source_name = "new_source"
    
    def fetch_articles(self) -> List[Dict]:
        # Implementation here
        pass
    
    def parse_article(self, raw_data: Dict) -> Article:
        # Parse raw article data into Article model
        pass
    
    def detect_department(self, article_data: Dict) -> Optional[str]:
        """
        Detect which department this article belongs to.
        
        Returns:
            Department name or None
        """
        content = f"{article_data.get('title', '')} {article_data.get('content', '')}"
        content_lower = content.lower()
        
        # Department keyword mapping
        department_keywords = {
            'music': ['ìŒì•…', 'ì‹¤ìš©ìŒì•…', 'ì„±ì•…', 'ì‘ê³¡', 'music'],
            'korean': ['í•œêµ­ì–´', 'êµ­ì–´êµ­ë¬¸', 'êµ­ë¬¸í•™', 'êµ­ì–´'],
            'english': ['ì˜ì–´', 'ì˜ì–´ì˜ë¬¸', 'ì˜ë¬¸í•™', 'english'],
            'liberal': ['ì¸ë¬¸', 'ì¸ë¬¸í•™', 'êµì–‘êµìœ¡', 'êµì–‘'],
        }
        
        for dept, keywords in department_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                return dept
        
        return None
```

### Adding New Source Configuration
1. Add source to `config/sources.yaml`:
```yaml
new_source:
  name: "New Source Name"
  url: "https://example.com"
  enabled: true
  scrape_interval: 3600  # seconds
  description: "Description of the source"
```

2. Register scraper in `core/scraper_factory.py`:
```python
from scrapers.new_source_scraper import NewSourceScraper

def create_scraper(source_name: str, config: dict) -> Optional[BaseScraper]:
    if source_name == "new_source":
        return NewSourceScraper(config)
    # ... existing scrapers
```

## ğŸ› Troubleshooting

### Selenium Issues
**ChromeDriver version mismatch**:
```bash
# Check versions
/usr/bin/google-chrome-beta --version
/usr/bin/chromedriver --version

# Should match major version (e.g., both 145.x)
```

**Solution**: Install matching ChromeDriver beta
```bash
sudo pacman -S chromedriver-beta
```

### No Articles Found
1. Check if cookies need acceptance (see SCRAPER_DEVELOPMENT_GUIDE.md)
2. View page source - is HTML minimal? â†’ Use Selenium
3. Test article URLs manually - do they work?
4. Check `selenium_page_source.html` debug file

### Empty Article Content
- Likely popup-based articles (Adiga pattern)
- URLs don't lead to content pages
- Must click links with Selenium to trigger popups

### Telegram Not Working
```bash
# Test bot token
curl "https://api.telegram.org/bot<YOUR_TOKEN>/getMe"

# Check logs
tail -f logs/monitor.log
```

## ğŸ¤– Telegram Integration

The system sends formatted Telegram messages:

```
ğŸµ [ìƒˆ ì…í•™ ê³µê³ ] ì„œìš¸ëŒ€í•™êµ ìŒì•…í•™ê³¼ ì¶”ê°€ëª¨ì§‘

ğŸ“Œ ë¶€ì„œ/í•™ê³¼: music
ğŸ“ ë‚´ìš©: ì„œìš¸ëŒ€í•™êµ ìŒì•…í•™ê³¼ì—ì„œ 2026í•™ë…„ë„ ì¶”ê°€ëª¨ì§‘ì„ ì‹¤ì‹œí•©ë‹ˆë‹¤...
ğŸ”— ë§í¬: https://adiga.kr/ArticleDetail.do?articleID=26546

#ëŒ€í•™ì…ì‹œ #music
```

## ğŸ“Š Current Data Sources

### Adiga (ì–´ë””ê°€)
* **URL**: https://www.adiga.kr
* **Status**: âœ… Active (Selenium + Popup Solution Working)
* **Coverage**: General admission news and announcements
* **Pattern**: JavaScript popups requiring Selenium click simulation

### Target Universities for Future Development
1. **ì„œìš¸ëŒ€í•™êµ** - https://admission.snu.ac.kr
2. **ì—°ì„¸ëŒ€í•™êµ** - https://admission.yonsei.ac.kr  
3. **ê³ ë ¤ëŒ€í•™êµ** - https://admission.korea.ac.kr
4. **í•œêµ­ëŒ€í•™êµ** - Individual department pages

## ğŸ¯ Architecture Status

### âœ… Completed Foundation
* **New Core Architecture**: `core/base_scraper.py`, `core/filter_engine.py`, `core/scraper_factory.py`
* **Enhanced Configuration**: YAML-based configs in `config/` directory
* **Data Models**: `models/article.py` for standardized article representation
* **State Management**: SQLite database for reliable state tracking
* **Safe GitHub Integration**: Interactive push scripts with review steps
* **Adiga Scraper**: Working Selenium implementation with popup handling

### ğŸ”„ In Progress
* **Content Discovery**: Finding actual admission announcement URLs on Adiga.kr
* **Template System**: Creating reusable scraper templates
* **Deadline System Integration**: Integrating deadline tracking into main architecture

### ğŸ“‹ Pending
* **Additional Sources**: Implementing scrapers for target universities
* **RSS Sources**: Adding RSS feed monitoring capabilities
* **Testing Framework**: Comprehensive test suite
* **Web Dashboard**: Monitoring status interface

## ğŸ”„ Upcoming: Deadline System Integration

The deadline tracking system will be integrated into the main architecture:

**Plan**:
- **Phase 1**: Fix admission scraper (âœ… Completed)
- **Phase 2**: Create `deadline_source.py` and integrate with monitor engine
- **Phase 3**: Unified notification pipeline for both admission news and deadlines

**Benefits**:
- Unified notification pipeline
- Shared duplicate detection
- Single database for all content
- Flexible scheduling options

## ğŸ“ˆ Future Enhancements

* **Web Dashboard**: Real-time monitoring status
* **Email Notifications**: Alternative to Telegram
* **REST API**: External integrations
* **Advanced Filtering**: ì§€ì—­, ì „í˜•ë³„, ëª¨ì§‘ì¸ì›
* **Mobile App**: Push notifications
* **Multi-language Support**: English/Korean interface
* **RSS Feed Support**: Monitor university RSS feeds
* **Automated Deadline Updates**: Scrape deadline information

## ğŸ¤ Contributing

1. **Read** `SCRAPER_DEVELOPMENT_GUIDE.md` first!
2. **Use** `scrapers/scraper_template.py` for new scrapers
3. **Test** thoroughly before submitting
4. **Document** any new patterns discovered

### Contribution Guidelines
* **New scrapers**: Follow `scraper_template.py` structure
* **Department keywords**: Add to `config/filters.yaml`
* **Testing**: Test with `python core/monitor_engine.py --test` before submitting
* **Documentation**: Update README with new source information

## ğŸ“– Documentation

- **SCRAPER_DEVELOPMENT_GUIDE.md** - Critical patterns and solutions for scraper development
- **scrapers/scraper_template.py** - Comprehensive template with examples
- **config/** - YAML configuration examples
- **DEADLINE_INTEGRATION_PLAN.md** - Plan for integrating deadline tracking

## ğŸ“„ License

MIT License

---

**Last Updated**: 08 February 2026  
**Maintainer**: tantry  
**Status**: âœ… Production Ready (Admission Scraper Working)  
**Next Phase**: Deadline System Integration & RSS Sources  
**GitHub**: https://github.com/tantry/uni_monitoring.kr
